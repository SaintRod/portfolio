[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Howdy.\nMy name is Santiago Rodriguez and I am a statistician, statistical programmer, machine learning practitioner, and a life-long learner.\nI enjoy traveling. Some of my favorite places to visit are Norway and Japan.\nThe best way to contact me is through LinkedIn."
  },
  {
    "objectID": "posts/other/other.html",
    "href": "posts/other/other.html",
    "title": "Other",
    "section": "",
    "text": "In this section you’ll find a collection of random things, from notes I took while reading a book or a video online.",
    "crumbs": [
      "Other"
    ]
  },
  {
    "objectID": "posts/other/study-guides/dsa.html",
    "href": "posts/other/study-guides/dsa.html",
    "title": "Data structures and algorithms",
    "section": "",
    "text": "About\nI find it helpful to make my notes available online so that I can access them from anywhere. The following notes are from lessons on data structures and algorithms.\n\ndata structures\nalgorithms\n\nThe course provided Python examples, and I replicated these examples using Julia and R.\n\nJulia\nPython\nR",
    "crumbs": [
      "Other",
      "Study Guides",
      "Data structures and algorithms"
    ]
  },
  {
    "objectID": "posts/projects/projects.html",
    "href": "posts/projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "This is the largest section, and the one hiring manager and recruiters will most likely be interested in.\nIn this section you’ll find code, output, and more! My interests span a wide range of topics, from building web apps to machine learning models.\nThe following are areas I’d like to explore further or projects I want to work on, presented in no particular order.\n\nRAG (LLMs)\ndeep learning from scratch (Keras, PyTorch, TensorFlow)\nauto diff (Jax, JuMP)\noptimization\nGenie (Julia)\nShiny for Python\nbuild an API (fastAPI, plumber)\ncausal inference (unsure yet how this differs from experimentation from a statistics perspective)\nsurvival analysis\na primer on driver analysis\nhighlighting the utility of categorical data analysis",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "posts/projects/time-series/household-forecasting.html",
    "href": "posts/projects/time-series/household-forecasting.html",
    "title": "Forecasting My Household’s Energy Consumption",
    "section": "",
    "text": "TL;DR\nI used my household energy consumption data to build forecasting models and put the models into production using APIs and docker.\n\n\nAbout\nWhen I worked at Deloitte we were tasked with creating a forecast for sales opportunities. After implementing a forecasting workflow on Databricks, I had an idea. I always have ideas.\nI proposed to my leaders that we refactor the code and architect a new solution. My idea was so modularize and generalize the code, so that it could easily extend to any configuration. By this I mean that eventually it wasn’t just the global CFO that wanted our forecast, but local member firm leaders as well. This project was one our teams biggest hits.\nThe down side to killer projects at work is that there isn’t anything tangible to show for them. So, I decided to replicate the architecture of the Deloitte project and improve it.\nOne thing I’ve noticed over the years is that much of the work data scientists create goes unused. I think this is why the machine-learning-engineer branch of data science formed. It’s not enough to create models. The missing piece was putting the models into production. That’s why in this project production is a key element.\n\n\nData\nI built a data pipeline for my household energy data and corresponding daily weather data. The data is housed in a private AWS S3 bucket.\nSee the post under “Data” called “Household Energy Consumption Data Pipeline” about the data pipeline for more info.\n\n\nPlanning\nAs of Oct 9th, 2024 this project is still in the planning phase because as my late father in-law used to say, before starting any project you should, “sit on your butt and think.”\nFor this project I will create forecasting models in Python and R. Yes, you read that correctly, this is a multilingual project! I am also exploring if Julia has time series forecasting capabilities. I started learning Julia c. 2021 and enjoy using it.\nI will use statistical, machine learning, and deep learning models. In Python, this will be accomplished via the Nixtla suite of packages. In R, I will use fable andmodeltimeviatidymodels`.\nThe challenge I’m thinking through is how to easily split training the models and forecasting. A neat trick that I came up with on the Deloitte project is to store all of the models in a module. This module can be imported into the training and forecasting workflows, respectively. The neat thing about this solution is that it avoids repetition. I also think it’s cleaner this way as everything is centralized.\nAn example project structure could be:\nproject-root\n    - py\n        - src\n            - models\n                - stats.py\n                - ml.py\n                - dl.py\n        - scripts\n            - training\n                - main.py\n            - forecasting\n                - main.py\n        - main.py\nIn this structure the models are modules that the training and forecasting workflows or pipelines leverage.\nThe main module will have a toggle to switch between training and forecasting. As this is a passion project and the forecasts aren’t used for anything, I don’t need to update the models frequently. I think that having main flip between training and forecasting is better than manually executing things. For example, ./py/main.py will import ./py/training/main.py and ./py/forecasting/main.py. Each of the children main files will be mini-control centers for their respective workflows.\nSome thought about production. My goal is to put this work into production, and for that I need a way to interact with the models easily. To accomplish this I want to create a fastAPI for the python forecasts and a plumber API for the R forecasts. I’ll likely package in a container using docker or podman.\nI’ll host the container on a spare desktop I’ve beefed up for these purposes - 16 cores, 64 GB RAM, and a NVIDIA GPU. When there’s new data and I want to obtain the latest forecasts I’ll spin up my spare desktop and ping it.\n\n\nCode\nCurrently, as the work is evolving the project is private. However, the goal is to make the code publicly available.",
    "crumbs": [
      "Projects",
      "Time Series",
      "Forecasting My Household's Energy Consumption"
    ]
  },
  {
    "objectID": "posts/projects/web-apps/web-apps.html",
    "href": "posts/projects/web-apps/web-apps.html",
    "title": "Web Apps",
    "section": "",
    "text": "Overview\n\nA Texas Parks and Wildlife resource, link\nA tool to help create stratified samples, , link",
    "crumbs": [
      "Projects",
      "Web Apps"
    ]
  },
  {
    "objectID": "posts/projects/web-apps/strat-sampling-tool.html",
    "href": "posts/projects/web-apps/strat-sampling-tool.html",
    "title": "Stratified Sampling",
    "section": "",
    "text": "About\nDuring my tenure at Consumers Energy, my sibling team was responsible for generating customer contact lists for Marketing teams. Mostly these lists were created via simple random sampling. However, there were some astute stakeholders that wanted to test if stratified sampling would yield more accurate results. Initially, the team accomplished this manually, but it was error-prone and onerous. So, I built a Shiny app for them to automate the task. Below is a replica of that work.\n\nwebapp\ncode",
    "crumbs": [
      "Projects",
      "Web Apps",
      "Stratified Sampling"
    ]
  },
  {
    "objectID": "posts/projects/analyses/analyses.html",
    "href": "posts/projects/analyses/analyses.html",
    "title": "Analyses",
    "section": "",
    "text": "Overview\n\nAn analysis of languages used to create statistics packages on Arxiv, link\nUsing Julia to analyze router packet data, link",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "posts/projects/data/household-energy-pipeline.html",
    "href": "posts/projects/data/household-energy-pipeline.html",
    "title": "Household Energy Consumption Data Pipeline",
    "section": "",
    "text": "Data\nThe smart meters can provide data as granular as 15-minute intervals. As you can imagine the data grows quickly.\nIn addition to the energy data I also work with weather data obtained via a NOAA API.\n\n\nData Pipeline\nMy first attempt at building a data pipeline was built in R and was wonky. I don’t remember when I started this project.\nSometime in Q3 2024 I decided to refactor the data pipeline. I still used R but I completely overhauled the code.\nThe goal of refactoring was to modernize the code, create modules, reproducibility, unit tests, leverage the cloud, and ease of use.\nOne of the main changes was the introduction of the box package.\nUsing box changed how I code in R. Normally, R users are taught to use library(pkg). But this is the antithesis many modern languages because library() imports all the namespaces. Working box makes coding in R like coding in Python or Julia. Since I learned how to use box I haven’t gone back.\nAnother change to how I used to write R code is leveraging base R more. When I learned R in 2014 all I knew was base R. Then over time I leaned more heavily on third-party packages, such as the Tidyverse. But, I’ve grown as a developer. I care much more about reproducibility now. And having tons of dependencies is risky.\nUsing box I created functions within modules and organized the modules by scope. For example, all AWS functions are housed in ./src/AWS.\nAnother sign of my maturity as a [statistical] programmer is testing. In R this done via the testthat package. I now test all of my functions. I do this even for analyses. How else can my work be trusted?\nThe data itself also received a face-lift. I updated how the data is stored. I now use Arrow, e.g., energy.arrow. And where the data is stored also changed. The data is exported locally and in the cloud. The data in the cloud is stored in a private AWS S3 bucket. The local files are backups.\nFinally, I used the R package docopt to create command line arguments. This makes executing the pipeline a breeze. For example, first I download the latest data from the website. I save the file in ./data/input. In the terminal I execute Rscript main.R --help if I forget which flags exist. To run the code: Rscript main.R -t -e -w. Where -t executes the tests. This ensures everything is up to snuff. The -e executes the energy pipeline. And the -w executes the weather pipeline.\nI recently found out a package called targets that is designed for pipelines. I may explore this in the future.\nOverall, I’m very happy with the refactoring.\n\n\nCode\nCurrently the code is private. If and when this changes I’ll update this page with a link to the repo.",
    "crumbs": [
      "Projects",
      "Data",
      "Household Energy Consumption Data Pipeline"
    ]
  },
  {
    "objectID": "posts/projects/stats/hypothesis-testing-primer.html",
    "href": "posts/projects/stats/hypothesis-testing-primer.html",
    "title": "A Primer on Hypothesis Testing",
    "section": "",
    "text": "Hypothesis testing\nDuring my tenure at Consumers Energy I was asked to help introduce the notion of testing. The focus was on marketing programs. This was a new-ish department and there were lots of opportunities.\nMy main focus was to build relationships with stakeholders and gently educate them about testing. Below is an example of some of the concepts and tools I used.\n\nreport",
    "crumbs": [
      "Projects",
      "Stats",
      "A Primer on Hypothesis Testing"
    ]
  },
  {
    "objectID": "posts/projects/tools/mult-lang-proj-template.html",
    "href": "posts/projects/tools/mult-lang-proj-template.html",
    "title": "A Language agnostic project template",
    "section": "",
    "text": "About\nI present a handy shell script to automate and standardize data analysis project structures. Yes, several language-specific templates exist, but using them assumes the language for the project has already been chosen. Often when planning a project, I don’t select a language until later in the planning stage. Also, much of a project’s structure is language-agnostic so there’s no need to wait to lay the foundation.\n\ncode",
    "crumbs": [
      "Projects",
      "Tools",
      "A Language agnostic project template"
    ]
  },
  {
    "objectID": "posts/viz/job-search-dashboard.html",
    "href": "posts/viz/job-search-dashboard.html",
    "title": "A Job Search Tracker",
    "section": "",
    "text": "In August my spouse and I welcomed a tiny human into our lives. Life was good. Then when I returned from parental leave I was informed that my team at Deloitte, about 20 people, would be let go.\nThis dashboard was created to track my job search post Deloitte. However, anyone can use the dashboard as the code is publicly available. The repo README provides instructions on how to do this. However, see below for a brief how-to.\n\ncode\ndashboard",
    "crumbs": [
      "Projects",
      "Visualizations",
      "A Job Search Tracker"
    ]
  },
  {
    "objectID": "posts/viz/job-search-dashboard.html#software",
    "href": "posts/viz/job-search-dashboard.html#software",
    "title": "A Job Search Tracker",
    "section": "1. Software",
    "text": "1. Software\nThe elements of the dashboard were created using Python 3.10.\nThe HTML output was created with Quarto - installation. Install the latest version as Quarto dashboards require a minimum version (1.4 I think).",
    "crumbs": [
      "Projects",
      "Visualizations",
      "A Job Search Tracker"
    ]
  },
  {
    "objectID": "posts/viz/job-search-dashboard.html#plot-backgrounds",
    "href": "posts/viz/job-search-dashboard.html#plot-backgrounds",
    "title": "A Job Search Tracker",
    "section": "2. Plot backgrounds",
    "text": "2. Plot backgrounds\nTo recreate the fun plot you will need two images:\n\nplot-background.png\nplot-point.png\n\nEnsure that the files are named exactly as the ones above.",
    "crumbs": [
      "Projects",
      "Visualizations",
      "A Job Search Tracker"
    ]
  },
  {
    "objectID": "posts/viz/job-search-dashboard.html#data",
    "href": "posts/viz/job-search-dashboard.html#data",
    "title": "A Job Search Tracker",
    "section": "3. Data",
    "text": "3. Data\n\nThe dashboard requires a CSV file saved in ./data/input\n\nYou can name the file whatever you want\nFor this example, let’s pretend the file is named elliott.csv\n\nThe file must contain the following columns\n\nThe data types are in the repo README\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\ncompany\nrole\nonsite\nhybrid\nremote\nsalary-low\nsalary-high\nrejected\ninterviewed\noffer\naccepted\n\n\n\n\n2024-10-09\nevil corp\nmr robot\n1\n\n\n50000\n1000000\n\n1\n1\n1",
    "crumbs": [
      "Projects",
      "Visualizations",
      "A Job Search Tracker"
    ]
  },
  {
    "objectID": "posts/viz/job-search-dashboard.html#python",
    "href": "posts/viz/job-search-dashboard.html#python",
    "title": "A Job Search Tracker",
    "section": "3. Python",
    "text": "3. Python\n\nopen a terminal and navigate to the project directory: cd ~/job-search\ncreate a virtual environment: python3.10 -m venv .venv\n\nI use pip, but uv is 🔥 right now (Oct 2024)\n\nactivate python source .venv/bin/activate\ninstall the project dependencies pip install -r requirements.txt\n\nNow, let’s check that everything is working. In the terminal run the following command to execute the testing logic: pytest.\nIf there are no errors then you’re safe to proceed to the next stage. If there are errors… open an issue 🤷",
    "crumbs": [
      "Projects",
      "Visualizations",
      "A Job Search Tracker"
    ]
  },
  {
    "objectID": "posts/viz/job-search-dashboard.html#quarto",
    "href": "posts/viz/job-search-dashboard.html#quarto",
    "title": "A Job Search Tracker",
    "section": "4. Quarto",
    "text": "4. Quarto\n\n_quarto.yml\n_quarto.yml is the control center of the Quarto project.\nHere is an example of a basic file. Customization is a bit outside our purposes. I encourage you to review the Quarto docs.\nproject:\n  type: website\n  output-dir: public\n\nwebsite:\n  title: \"My Jobs Dashboard\"\n  site-url: \"\"\n  search: false\n  page-footer: \n    background: dark \n    right: Built with Quarto\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: false\n\n\nHosting/ Publishing or GitHub vs GitLab\nThe dashboard can be published on either GitHub or GitLab. GitHub is a little easier and the Quarto docs illustrate how to publish on GitHub.\nI rendered this dashboard on GitLab because I’ve never done that before and I wanted to learn how.\nThe _quarto.yml specifies where to render the dashboard. Notice the output-dir under project.\n\non GitLab Pages the output dir is ./public\non GitHub Pages the output dir is ./docs\n\nUpdate output-dir accordingly.\nOne last note on GitLab Pages. GitLab uses runners to publish the dashboard, while GitHub has a GUI option.\nTo use GitLab you need a file named .gitlab-ci.yml. Contents below.\nimage: alpine:latest\n\nstages:\n- test\n- deploy\n\nsast:\n  stage: test\ninclude:\n- template: Security/SAST.gitlab-ci.yml\n\npages:\n  stage: deploy\n  script:\n    - echo \"Deploying GitLab Pages from the public directory\"\n  artifacts:\n    paths:\n      - public \n  only:\n    - main\n\n\nRendering\nAssuming you installed Quarto, let’s render the output.\nIn the terminal, type either:\n\nquarto render - generates output in the specified directory\nquarto preview - reactive programming so that changes are rendered live\n\nPreview is helpful if you’re actively working on stuff. When you preview the dashboard will open in a browser.\nWhile render only produces output. To view the output go to ./public/index.html on GitLab or ./docs/index.html on GitHub.",
    "crumbs": [
      "Projects",
      "Visualizations",
      "A Job Search Tracker"
    ]
  },
  {
    "objectID": "posts/talks/talks.html",
    "href": "posts/talks/talks.html",
    "title": "Talks",
    "section": "",
    "text": "In this section you’ll find speaking engagements. While there aren’t many, it’s something I wish to more of.",
    "crumbs": [
      "Talks"
    ]
  },
  {
    "objectID": "posts/blogs/blogs.html",
    "href": "posts/blogs/blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "I enjoy writing. But I don’t enjoy paywalls. So, I write on open platforms, such as GitHub Pages or Quarto Pubs.\nIn this section you’ll find collections of writings. Enjoy!\n\nFunctional data analysis\nPersonal blog about data stuff",
    "crumbs": [
      "Blogs"
    ]
  },
  {
    "objectID": "posts/blogs/fda.html",
    "href": "posts/blogs/fda.html",
    "title": "Functional Data Analysis",
    "section": "",
    "text": "About\nSince first learning about splines c.2020 I have been fascinated with functional data analysis (FDA).\nOver the years I’ve explored applications of FDA at work and for personal projects. I created this blog to collect these works and house them under one roof. The blog also provides me an avenue to share thoughts about the work that might not fit into the work itself, such as how the document was created, why I used certain software, etc.\nThe blog can be accessed here.",
    "crumbs": [
      "Blogs",
      "Functional Data Analysis"
    ]
  },
  {
    "objectID": "posts/blogs/personal.html",
    "href": "posts/blogs/personal.html",
    "title": "My Personal Blog",
    "section": "",
    "text": "About\nThis blog is a place for me to write about stuff.\nIn particular, I write about data stuff. For example, I may write a series about one of my passion projects. I might write something about data science in the wild.\nPlease take a look, I would appreciate it.",
    "crumbs": [
      "Blogs",
      "My Personal Blog"
    ]
  },
  {
    "objectID": "posts/talks/r-in-energy.html",
    "href": "posts/talks/r-in-energy.html",
    "title": "R in Energy",
    "section": "",
    "text": "About\nSome time ago I had the great pleasure to share some of my work on functional data analysis.\nThe R in Energy meetup is a industry-focused data meetup for professionals working with R in the energy sector/ utilities industry. The meetups are hosted by Posit.\n\n\nLinks\n\nYouTube\nPresentation\n\n\n\nEmbedded\n\n\n\n\nR in Energy",
    "crumbs": [
      "Talks",
      "R in Energy"
    ]
  },
  {
    "objectID": "posts/viz/viz.html",
    "href": "posts/viz/viz.html",
    "title": "Visualizations",
    "section": "",
    "text": "Overview\n\nA dashboard to track my job search in 2024, link",
    "crumbs": [
      "Projects",
      "Visualizations"
    ]
  },
  {
    "objectID": "posts/projects/tools/tools.html",
    "href": "posts/projects/tools/tools.html",
    "title": "Tools",
    "section": "",
    "text": "Overview\n\nA shell script that creates data analysis project templates, link",
    "crumbs": [
      "Projects",
      "Tools"
    ]
  },
  {
    "objectID": "posts/projects/stats/stats.html",
    "href": "posts/projects/stats/stats.html",
    "title": "Statistics",
    "section": "",
    "text": "Overview\n\nA primer on hypothesis testing, link",
    "crumbs": [
      "Projects",
      "Stats"
    ]
  },
  {
    "objectID": "posts/projects/data/data.html",
    "href": "posts/projects/data/data.html",
    "title": "Data",
    "section": "",
    "text": "About\nThis section describes data engineering projects.\nWhile I have mostly focused on data science work, I have done plenty of data wrangling. Since I love to learn I’ve often volunteered to assist the data engineers when they’ve asked for extra help.\n\nA data pipeline for projects using my household energy consumption, link",
    "crumbs": [
      "Projects",
      "Data"
    ]
  },
  {
    "objectID": "posts/projects/analyses/internet-outage.html",
    "href": "posts/projects/analyses/internet-outage.html",
    "title": "Internet Outage Detection",
    "section": "",
    "text": "About\nYears ago I had a faulty internet router. The problem was I didn’t know it was the router yet. Eventually I set up a shell script to ping the router and log results. I analyzed the results and discovered that the router was the issue. I replaced the router with a Gryphon router - if you haven’t heard of it, I highly recommend it. Additionally, this was my first foray into the work of programming with Julia.\n\nreport",
    "crumbs": [
      "Projects",
      "Analyses",
      "Internet Outage Detection"
    ]
  },
  {
    "objectID": "posts/projects/analyses/arxiv-stats-pkgs.html",
    "href": "posts/projects/analyses/arxiv-stats-pkgs.html",
    "title": "Arxiv Packages Analysis",
    "section": "",
    "text": "About\nAn analysis of Julia, Python, and R packages on the statistics section of Arxiv. The term “R package” had the most results, followed by Python then Julia.\n\nreport\ncode",
    "crumbs": [
      "Projects",
      "Analyses",
      "Arxiv Packages Analysis"
    ]
  },
  {
    "objectID": "posts/projects/web-apps/tpwd.html",
    "href": "posts/projects/web-apps/tpwd.html",
    "title": "Texas Parks & Wildlife",
    "section": "",
    "text": "Years ago when I moved to Texas from South Florida I missed going fishing. I discovered that during winter months, Texas Parks & Wildlife (TPWD) stocks local fishing areas with trout. This was music to my ears. However, the static HTML table provided by TPWD wasn’t conducive to getting outdoors. The problem with the static table is it only provides the fishing area and the city, and it’s too tedious to search each location individually on Google Maps for example. So, I built a Shiny app for myself and hosted it on ShinyApps.io (link). The app was successful and I was able to go fishing. The main goal of this version was to help me find the nearest fishing location and to be able to sort and filter locations.\nRecently, I decided to learn more about putting a Shiny app in production. I decided to focus on the TPWD trout POC I built years ago because I thought others could benefit from having a better interface to the trout stocking data. The scope of the app expanded beyond just the trout stocking data though as I realized that other TPWD resources were useful but not interactive. The static nature of the resources might deter some people from getting outdoors. So, I built this Shiny app.\nThe goal of this app is to help Texans interact with Texas Parks & Wildlife static online resources and to help Texans get outdoors.\n\nweb app code\ndata layer code\nwebsite\nShiny contest 2024\n\nThe web app is hosted on AWS using their free tier. Once the 12-months free expires, depending on the cost to maintain the website the link above my not exist. However, the code is readily available.\n\n\nFunctionality:\n\nInteractive tables\nCalculate drive time and distance\n\nTables:\n\nBoat ramps\nCatfish stocking information\nParks (list of state parks)\nTrout stocking information (new season begins in November)\nWildlife management areas\n\nHosting:\n\nAWS EC2 (free tier t2.micro)\n\nSoftware:\n\nShiny (R) built with the Rhino development framework\nHERE API for route calculation logic\nDocker",
    "crumbs": [
      "Projects",
      "Web Apps",
      "Texas Parks & Wildlife"
    ]
  },
  {
    "objectID": "posts/projects/web-apps/tpwd.html#technical-details",
    "href": "posts/projects/web-apps/tpwd.html#technical-details",
    "title": "Texas Parks & Wildlife",
    "section": "",
    "text": "Functionality:\n\nInteractive tables\nCalculate drive time and distance\n\nTables:\n\nBoat ramps\nCatfish stocking information\nParks (list of state parks)\nTrout stocking information (new season begins in November)\nWildlife management areas\n\nHosting:\n\nAWS EC2 (free tier t2.micro)\n\nSoftware:\n\nShiny (R) built with the Rhino development framework\nHERE API for route calculation logic\nDocker",
    "crumbs": [
      "Projects",
      "Web Apps",
      "Texas Parks & Wildlife"
    ]
  },
  {
    "objectID": "posts/projects/time-series/time-series.html",
    "href": "posts/projects/time-series/time-series.html",
    "title": "Time Series",
    "section": "",
    "text": "About\nThroughout my time as a data scientist I’ve been tasked with forecasting projects. While it’s fun to talk about these projects, I can’t share any output or code.\nThe projects below are passion projects to illustrate my knowledge and capabilities.\n\nForecasting my household’s energy consumption, link",
    "crumbs": [
      "Projects",
      "Time Series"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#variable-types-and-terminology",
    "href": "posts/other/study-guides/elements-stat-learning.html#variable-types-and-terminology",
    "title": "The Elements of Statistical Learning",
    "section": "2.2 Variable types and terminology",
    "text": "2.2 Variable types and terminology\n\n\\(\\vec{Y}\\): response variable, dependent variable, outcome, regressor, target\n\\(X\\): feature matrix, predictors, independent variables\n\nQualitative \\(\\vec{Y}\\): categorical (nominal, ordinal), discrete, or factors (with \\(f\\) levels)\nQuantitative \\(\\vec{Y}\\): regression Qualitative \\(\\vec{Y}\\): classification\nBinary qualitative \\(\\vec{x}\\): usually coded as {0, 1} | {-1, 1}\nQualitative \\(\\vec{x}\\) with \\(\\gt 2\\) levels: dummy variables (where \\(l-1\\) variables are created)",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#two-simple-approaches-least-squares-vs-nearest-neighbors",
    "href": "posts/other/study-guides/elements-stat-learning.html#two-simple-approaches-least-squares-vs-nearest-neighbors",
    "title": "The Elements of Statistical Learning",
    "section": "2.3 Two simple approaches: least squares vs nearest neighbors",
    "text": "2.3 Two simple approaches: least squares vs nearest neighbors\nFirst implicit mention of bias variance tradeoff. The more flexible a model, the lower the bias but higiher the variance. Vice versa, the more inflexible a model, the greather the bias but lower the variance.\n\n2.3.1 Linear models and least squares\n\\(\\hat{Y} = \\hat{\\beta_0} + \\sum_{j=1}^{p} X_j \\hat{\\beta_j}\\)\nor\n\\(\\hat{Y} = X^{\\top} \\hat{\\beta}\\)\n\n\\(\\hat{\\beta_0}\\): intercept or bias (in ML lore)\n\nMost common way to fit the linear model is least squares.\nThe task is to minimize the residual sum of squares (RSS):\n\\(RSS(\\beta) = \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\\)\nDegress of freedom: \\(N - p\\)\n\n\n2.3.2 Nearest-neighbor methods\n\\(\\hat{Y} = \\frac{1}{k} \\sum_{x_i \\in N_k (x)} y_i\\)\nWhere \\(N_k (x)\\) = neighborhood of \\(x\\) defined by \\(k\\) closest observations.\nDegress of freedom: \\(\\frac{N}{k}\\)\nFirst mention of not using training data to assess fit.",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#statistical-decision-theory",
    "href": "posts/other/study-guides/elements-stat-learning.html#statistical-decision-theory",
    "title": "The Elements of Statistical Learning",
    "section": "2.4 Statistical decision theory",
    "text": "2.4 Statistical decision theory\n\\(X\\) and \\(\\vec{Y}\\) ~ \\(P(X, Y)\\) = a joint distribution.\nThe task is to \\(\\approx\\) a function \\(f(X)\\) for prediction \\(Y\\) given the values of \\(X\\).\nLoss function: \\(L(Y, f(x))\\)\nThe most common loss function is squared error loss: \\((Y - f(x))^2\\)\n\nknown as the \\(L_2\\) loss\nmean\noptimization: minimize: \\(argmin_c E_{Y | X} ([Y-c]^2 | X = x)\\)\nsolution to ^ = the regression function or conditional expectation: \\(f(x) = E(Y | X = x)\\)\nthe best prediction for \\(Y\\) for any \\(X = x\\) is the conditional mean when measured by average squared error\n\nWhy not alwasys use the most flexible model? If the data is best approximated by a linear, parametric, or inflexible model is more appropriate then can usually get more stable estimates.\nFirst mention of the curse of dimensionality. For nearest neighbors methods, as \\(p\\) - number of predictors - increases, the number of observations at \\(k\\) decreases. The asymptotic approximations still hold, but the rate at which convergence occurs slows.\nUltimately, both the model-centric or parametric and nearest neighbors approachs \\(\\approx\\) the conditional expectations via averages.\n\\(L_1 = |Y - \\hat{Y}|\\) = median(\\(Y | X = x\\)) - the median of the conditional expectation.\n\nmore robust than \\(L_2\\) loss, i.e., less sensitive to extreme values\nnon-convex optimization problem,i.e., discontinuities in the derivative\n\n……………………..\nWhat about qualitative \\(Y\\)?\n\nzero-one loss: missiclassifications are coded as 1\n\nEPE = \\(E[L(G, \\hat{G}(X))]\\)\n\\(\\hat{G} = argmin_{g \\in G} [1 - P(g | X = x)]\\)\n\nor, \\(\\hat{G} = G_k\\) if \\(P(G_k | X = x) = max_{g \\in G} P(g | X = x)\\)\n\n\n\nThis is known at the Bayes classifier: classify to the most probable class The error rate for the Bayes classifier = Bayes rate.",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#local-methods-in-high-dimensions",
    "href": "posts/other/study-guides/elements-stat-learning.html#local-methods-in-high-dimensions",
    "title": "The Elements of Statistical Learning",
    "section": "2.5 Local methods in high dimensions",
    "text": "2.5 Local methods in high dimensions\nFormally discusses the curse of dimensionality.\nHypothetical scenario for nearest neighbor. Assume the data is uniformily distrubuted in a p-dimensional unit hypercube (figure 2.6 p. 23). The notion of local average breaks down quickly. For example, in 10 dimensions (\\(p\\) = 10) to capture 1% or 10% of the data for a local average, need 63% or 80% of the range of each predictor.\nAnother issue with high dimensions is that all obsevations are near an edge (and far away from other observations). This is an issue because prediction is harder at the boundries.\nSampling density is proportional to \\(N^{\\frac{1}{p}}\\) where \\(p\\) = dimension of the input space (i.e., number of columns in \\(X\\)). If \\(p\\) = 1 and \\(N\\) = 100, then to acheive the same sampling density the sample size required is \\(N_10 = 100^{10}\\).\n……………………..\nBias-variance decomposition AKA bias-variance tradeoff.\nMSE = Mean Squared Error\n\n\\(E[y - \\hat{y}]^2\\)\n\\(E[\\hat{y} - E(\\hat{y})]^2 + [E(\\hat{y}) - y]^2\\)\nVar(\\(\\hat{y}\\)) + Bias\\(^2(\\hat{y})\\)\nVar + Bias\\(^2\\)\n\nTo estiamte functions with many variables, need large training data. More variables, more data.\n……………………..\nWhen the assumptions are apporpriate, the linear model is unbiased (BLUE = best linear unbiased estimator). That is, MSE only depends on the variance term. If, however, the assumptions are inapporpriate, then the model is biased.",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#statistical-models-supervised-learning-and-function-approximation",
    "href": "posts/other/study-guides/elements-stat-learning.html#statistical-models-supervised-learning-and-function-approximation",
    "title": "The Elements of Statistical Learning",
    "section": "2.6 Statistical models, supervised learning, and function approximation",
    "text": "2.6 Statistical models, supervised learning, and function approximation\n\n2.6.1 A statistical model for the join distribution \\(P(X, Y)\\)\nThe additive model: \\(Y = f(X) + \\epsilon\\)\nAdditive in the sense that the error term is added to the function results.\nThis form is not usually applied to qualitative \\(Y\\).\n\n\n2.6.2 Supervised learning\nThe supervised learning paradigm.\nLearn \\(f()\\) by example through a teacher.\nUses a training set as a sample of the full data.\nThe learning algorithm learns by example.\n\n\n2.6.3 Function approximation\nComputer science -&gt; supervised learning paradigm\nWhile applied math and statistics -&gt; function approximation and estimation.\nSR: Leo B. “the two cultures”\nIt can be useful to approach supervised learning as a problem in funciton approximation because it encourages geometrical concepts. This is the approach of this book.\nAnother class of useful approximators is the linear basis expansion: \\(f_{\\theta}(x) = \\sum_{i=1}^{K} h_k (x) \\theta_k\\)\nWhere \\(h_k()\\) are functions or transformations of the input vector \\(\\vec{x}\\). Examples: polynomial or trigonometric expansions, and nonlinear expansions such as the sigmoid transformation: \\(\\frac{1}{1 + e^{-x}}\\)\n……………………..\nLeast squares is convenient it is not the only criteria. There are also iterative and numerical optimization methods.\nA more general estimation method is maximum likelihood estimation.\n\n\\(L(\\theta)\\) = Likelihood function\nAssumes that the most reasonable (read likely) values for \\(\\theta\\) [wrt to the Likelihood function] are those for which the probability of the observed sample is largest\nLeast squares is a special case of MLE that uses the conditional likelihood: \\(P(Y | X, \\theta) = N(f_{\\theta}(X), \\sigma^2)\\)\nAnother example is the multinomial likelihood function \\(P(G | X)\\)\n\nlog-likelihood: \\(L(\\theta) = \\sum_{i=1}^{N} log \\ p_{g_i, \\theta} (x_i)\\)\nAlso known as cross-entropy loss",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#structured-regression-models",
    "href": "posts/other/study-guides/elements-stat-learning.html#structured-regression-models",
    "title": "The Elements of Statistical Learning",
    "section": "2.7 Structured regression models",
    "text": "2.7 Structured regression models\n\n2.7.1 Difficulty of the problem\nThere may exist infinitely many solutions to the optimization probelm (i.e., minizing the loss function). It’s useful to enforce restrictions. The restrictions are sometimes encoded into the parametric function \\(\\hat{f}()\\) or may be built into the learning method itself. Most restrictions can be described as complexity restrictions - usually means some kind of regular behavior in small neighborhoods of the input space.\nThe strength of the constraint depends on the neighborhood size. The larger the neighborhood, the stronger the constraint, and more sensitive the solution is to the choice of constraint. For example, local linear fits on tiny neighborhoods is [practically] no constraint at all. While local linear fits in large neighborhoods is \\(\\approx\\) globally linear model and is very restrictive.\nTakeaway: any method that attempts to produce locally varying functions in small isotropic (read homogeneous) neighborhoods will struggle in high dimensions\n\nisotropic: (of an object or substance) having a physical property which has the same value when measured in different directions",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#classes-of-restricted-estimators",
    "href": "posts/other/study-guides/elements-stat-learning.html#classes-of-restricted-estimators",
    "title": "The Elements of Statistical Learning",
    "section": "2.8 Classes of restricted estimators",
    "text": "2.8 Classes of restricted estimators\nEach class has parameters called smoothing parameters that control the size of the local neighborhood.\n\n2.8.1 Roughness penalty and Bayesian methods\nPenalizes RSS -&gt; PRSS: \\(PRSS(f; \\lambda) = RSS(f) + \\lambda J(f)\\)\n\n\\(J(f)\\) = penalty functions AKA regularization methods\n\n\\(J(f)\\) is a user-selected function\n\\(J(f)\\) will be large for functions that vary rapidly over small regions of input space\ntypes\n\nadditive penalties are used with additive functions\n\n\\(J(f)\\) corresponds to a log-prior; PRSS -&gt; log-posterior distribution\n\nminimizing PRSS = finding the posterior mode\n\n\n\n\n\n2.8.2 Kernel methods and local regression\nSpecify the nature of the local neighborhood and of the class of regular functions fitted locally.\nThe local neighborhood is specified by the kernel function \\(K_{\\lambda}(x_0, x)\\)\n\nassigns weights to points x in a region around \\(x_0\\)\nexamples\n\nGuassian kernal leverages the Guassian [probability] density function\nNadaraya-Watson weighted average\n\n\n\n\n2.8.3 Basis functions and dictionary methods\nBasis methods AKA dictionary methods\n\npolynomial expansion\nlinear expansion: \\(f_\\theta(x) = \\sum_{m=1}^M \\theta_m h_m(x)\\)\n\n\\(h_m\\) is a function of the input \\(x\\)\nlinear in regards to the parameter \\(\\theta\\)\n\nradial basis functions\n\nsymmetric \\(p\\)-dimensional kernels located at particlar centroids: \\(f_\\theta(x) = \\sum_{m=1}^M K_{\\lambda_m} (\\mu_m, x) \\theta_m\\)\ncentroids \\(\\mu_m\\), scales \\(\\lambda_m\\)\n\n\nFor kernel methods, the [hyper] parameters convert the optimization problem from a straightforward linear problem into a combinatorially hard nonlinear problem.\nsingle-layer feed-forward neural network with linear output weights is similar to an adaptive basis function: \\(f_\\theta(x) = \\sum_{m=1}^M \\beta_m \\sigma (\\alpha^T_m x + b_m)\\). Where \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\) is known as the activation function",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/elements-stat-learning.html#model-selection-and-the-bias-variance-tradeoff",
    "href": "posts/other/study-guides/elements-stat-learning.html#model-selection-and-the-bias-variance-tradeoff",
    "title": "The Elements of Statistical Learning",
    "section": "2.9 Model selection, and the bias-variance tradeoff",
    "text": "2.9 Model selection, and the bias-variance tradeoff\nCan’t use the training data to estimate the smoothing or complexity parameters because the solution is always to overfit to the data, i.e., choose the smallest neighborood.\nTest or generalization error.\nTypes of errors:\n\nirreducible error - can’t do anything about this\nbias - \\(Y - \\hat{Y}\\) or more statistically \\(E[Y] - E[\\hat{Y}]\\); how off the estimator is from the truth/ true mean\nvariance - \\(\\sigma^2\\)\n\nBias-variance tradeoff - as model complexity increases the variance tends to increase while the bias decreases.\nSR: Implies there’s a relationship between complexity and overfitting",
    "crumbs": [
      "Other",
      "Study Guides",
      "The Elements of Statistical Learning"
    ]
  },
  {
    "objectID": "posts/other/study-guides/study-guides.html",
    "href": "posts/other/study-guides/study-guides.html",
    "title": "Study Guides",
    "section": "",
    "text": "I love to learn. Truly. According to Gallop Strengths, Strengths Finder, or whatever it’s called now Learner is my top strength. I’ve taken the assessment multiple times over the years and Learner is always on top.\nAs a Learner, I often read technical books. When I read such books I usually take notes in case I need to revisit the key takeaways. This is a landing page for my notes.\n\nDSA\nThe Elements of Statistical Learning",
    "crumbs": [
      "Other",
      "Study Guides"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Thank you for visiting!",
    "section": "",
    "text": "UAI"
  }
]